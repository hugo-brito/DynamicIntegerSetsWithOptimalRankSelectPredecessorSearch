\chapter{Conclusion} \label{sec:conclusionChapter}

We conclude this report by highlighting the contributions of this project:
\begin{itemize}
    \item
    Within the scope defined in Section~\ref{sec:scope}, we have explained the data structure presented in \cite{patrascu2014dynamic}, together with helper functions and algorithms. This has been complemented with illustrative examples.
    
    \item
    We have implemented and documented the data structure, making it publicly available for future work.
    
    \item
    We have implemented and included correctness tests in the repository. These attest to the correctness of the implementations we present and that were designed such that they can be used to assess the correctness of further implementations.
    
    \item
    We have listed other data structures that are of interest within the context of the dynamic predecessor problem, pointing to future work in the form of benchmarks.
\end{itemize}

\section{Final Remarks}

\begin{itemize}
    \item
    There is a fine balance between space and time consumption when developing and implementing sub-logarithmic data structures and algorithms. For example, van Emde Boas trees is a very fast data structure, but with a big space consumption drawback. Fusion Trees improve on space consumption, but updates take excessive time. Dynamic Fusion Trees, introduced in this project, seem to find the perfect balance between space and time consumption.
    
    \item
    As mentioned in Section~\ref{sec:modelsofcomputation}, theoretical bounds have the potential to hide big constants.
    
    This project has been about implementing algorithms that use a constant number of operations to compute the result, assuming that these are an improvement in comparison with logarithmic algorithms (such as binary search).
    We want to highlight that despite the fact that it is plausible to implement the \textit{Dynamic Fusion Node} using only $O(1)$ operations of the word RAM model, the hidden constant might not be negligible.
    
    For instance, the {\ttfamily DynamicFusionNodeBinaryRank} implementation from Section~\ref{sec:DynamicFusionNodeBinaryRank} computes $\text{rank}(x)$ with binary search.
    In this particular implementation, and because we set $w = 64$, allowing us to store at most $k = 16$ keys, we know that the worst-case running time of a $\text{rank}(x)$ query will be when the node is full, e.g., $n = k = 16$.
    Computing $\log_2 16 = 4$.
    We conclude that, when this implementation is full, it will take at most $4$ iterations for rank to compute the answer.
    
    The {\ttfamily DynamicFusionNodeDontCaresRank} implementation from Section~\ref{sec:rankWithDontCares} uses $O(1)$ operations to compute a $\text{rank}(x)$ query. The constant number of operations used by this method is larger than the $4$ iterations taken by the binary search rank algorithm from Section~\ref{sec:DynamicFusionNodeBinaryRank}.
    The {\ttfamily match} subroutine alone, used by the rank with "don't cares" algorithm, uses $5$ word RAM operations, and the whole rank algorithm needs to run this subroutine at least once; and if the key is not in the set, it needs to run it once again. That is already $10$ operations, not counting with some other necessary subroutines such as the select query and finding the branching bit, $j$.
    
    Our conclusion is that $w = 64$ is potentially a very small word size to take advantage of the running time of these algorithms.
\end{itemize}

\section{Future Work} \label{sec:futureWork}
In this section, we leave some suggestions on how further work on this topic can be conducted. We have split these into three main categories:
\begin{enumerate}
    \item
    Implementation, which covers specific data structures or algorithms.
    
    \item
    Optimization, which entails improving the present code.
    
    \item
    Benchmarking, which points to other data structures that either solve partially or totally the dynamic predecessor 
    
\end{enumerate}

\subsection{Implementation} \label{sec:FutureWorkImplementation}

\paragraph*{Delete methods}
The implementation featured in Section~\ref{sec:InsertDontCares} implements the {\ttfamily delete} method naively, but all the ingredients needed to implement it while adhering to \textit{Inserting a key} section of \cite{patrascu2014dynamic} are already present in the implementation.

\paragraph*{Key compression in constant time}
One of the bottlenecks of implementations from Sections~\ref{sec:rankWithDontCares} and \ref{sec:InsertDontCares} is how the compressed keys with "don't cares" are maintained. Specifically, the next iterative step in the implementation from Section~\ref{sec:InsertDontCares} is to implement functions that compute the compressed keys in $O(1)$ time, as explained in Chapters $[3.2 - 3.3]$ of \cite{patrascu2014dynamic}.

\paragraph*{Dynamic Fusion Tree}
After enabling all the operations at the node level in $O(1)$ time with the previous steps' implementation, all that remains to complete the implementation is to implement a $k$-ary tree using a \textit{Dynamic Fusion Node} as its node. This is covered in Chapter $4$ of \cite{patrascu2014dynamic}.

\paragraph*{Non-recursive implementation}
Chapter 4 of \cite{patrascu2014dynamic} mentions that once the dynamic fusion tree is implemented, the rank operation on a tree is given by a recursive function. Recursion in Java can be slow \cite{shirazi2003java}, and for this reason, a non-recursive alternative is preferred.

\subsection{Benchmarking}

\paragraph*{msb functions}
It would be interesting to see how the different msb functions implemented in this project compare to each other in terms of time.

\paragraph*{Dynamic Predecessor Problem Data Structures}
Once the \textit{Dynamic Fusion Tree} is fully implemented, the next logical step would be to benchmark it with other data structures that solve, either partially or entirely, the dynamic predecessor problem. These would be the data structures mentioned in Section~\ref{sec:IntegerSets}.

\subsection{Optimization}

\paragraph*{Space}
Space consumption can be improved by, for instance, avoiding and combining some of the fields. When $k = 8$, $bKey$ would only use $8$ of the $64$ allocated bits, and $index$ would use $3 \times 8 = 24$ bits. So, in total, $32$ bits for both words. This is an example of how space can be improved.

\paragraph*{Simulate a longer word size}
One of the limitations of the present implementation is that the maximum integer size is 64 bits on a common modern machine. An interesting improvement would be to simulate larger word size $w$, which would increase $k$, allowing to store more keys at the node level. This would have to be carefully studied to keep every operation in $O(1)$ time.


\paragraph*{Lookup {\ttfamily M} constant}
The {\ttfamily Util} class includes a function, {\ttfamily M}, which takes $b$ and $w$ as parameters and returns the word $(0^{b-1}1)^{(w/b)}$. It is a widely used function in this repository, and it has been implemented with a loop. With limited word size, this could easily be implemented as a lookup function, improving the running time up to $O(1)$.

\paragraph*{Profiling}
This technique allows us to see which sections of the code the CPU uses most of the time, indicating possible code bottlenecks. By using it, the implementation can be fine-tuned up to the point where it can be competitive.